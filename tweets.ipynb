{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tweets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "import re\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data preparation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative_df shape: (1117, 1)\n",
      "neutral_df shape: (1570, 1)\n",
      "positive_df shape: (1186, 1)\n",
      "df shape: (3873, 2)\n",
      "count types\n",
      " 0    1570\n",
      " 1    1186\n",
      "-1    1117\n",
      "Name: type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_SEPARATOR = 'ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡'\n",
    "\n",
    "TYPE_NEGATIVE = -1\n",
    "TYPE_NEUTRAL = 0\n",
    "TYPE_POSITIVE = 1\n",
    "\n",
    "negative_df = pd.DataFrame(pd.read_csv('./data/processedNegative.csv').columns, columns=['text'])\n",
    "neutral_df = pd.DataFrame(pd.read_csv('./data/processedNeutral.csv').columns, columns=['text'])\n",
    "positive_df = pd.DataFrame(pd.read_csv('./data/processedPositive.csv').columns, columns=['text'])\n",
    "\n",
    "print(f'negative_df shape: {negative_df.shape}')\n",
    "print(f'neutral_df shape: {neutral_df.shape}')\n",
    "print(f'positive_df shape: {positive_df.shape}')\n",
    "\n",
    "negative_df['type'] = TYPE_NEGATIVE\n",
    "neutral_df['type'] = TYPE_NEUTRAL\n",
    "positive_df['type'] = TYPE_POSITIVE\n",
    "df = pd.concat([negative_df, positive_df, neutral_df])\n",
    "\n",
    "print(f'df shape: {df.shape}')\n",
    "print('count types')\n",
    "print(df['type'].value_counts())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Drop duplicates"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "def drop_duplications_preparation(tweet):\n",
    "    tweet_wituot_digits = re.sub(r\"\\d+\", '', tweet.lower())\n",
    "    tweet_wituot_digits_and_punctuation = re.sub(r'[^\\w\\s]', '', tweet_wituot_digits)\n",
    "    tweet_wituot_digits_and_punctuation_and_add_spaces =  re.sub(' +', ' ', tweet_wituot_digits_and_punctuation)\n",
    "    return tweet_wituot_digits_and_punctuation_and_add_spaces.strip()\n",
    "\n",
    "df['text'] = df['text'].apply(drop_duplications_preparation)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df shape: (3417, 2)\n",
      "count types\n",
      " 0    1470\n",
      "-1     974\n",
      " 1     973\n",
      "Name: type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = df.drop_duplicates()\n",
    "\n",
    "print(f'df shape: {df.shape}')\n",
    "print('count types')\n",
    "print(df['type'].value_counts())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df count types\n",
      "(2733, 2)\n",
      " 0    1184\n",
      " 1     779\n",
      "-1     770\n",
      "Name: type, dtype: int64\n",
      "test_df count types\n",
      "(684, 2)\n",
      " 0    286\n",
      "-1    204\n",
      " 1    194\n",
      "Name: type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2)\n",
    "\n",
    "print('train_df count types')\n",
    "print(train_df.shape)\n",
    "print(train_df['type'].value_counts())\n",
    "print('test_df count types')\n",
    "print(test_df.shape)\n",
    "print(test_df['type'].value_counts())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Crete preprocessing methods"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18 µs, sys: 1 µs, total: 19 µs\n",
      "Wall time: 24.1 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from tokenizer import remove_stop_words, stem_tokens, lemmatize_tokens, spell_tokens, get_tokenizer\n",
    "\n",
    "\n",
    "preprocessing_methods = {\n",
    "    \"Just tokenized\":                                     [],\n",
    "    \"Stemmed\":                                            [stem_tokens],\n",
    "    \"Stemmed (stopword removed)\":                         [stem_tokens, remove_stop_words],\n",
    "    \"Lemmatized\":                                         [lemmatize_tokens],\n",
    "    \"Lemmatized (stopword removed)\":                      [lemmatize_tokens, remove_stop_words],\n",
    "    \"Spelled\":                                            [spell_tokens],\n",
    "    \"Spelled (stopword removed)\":                         [spell_tokens, remove_stop_words],\n",
    "    \"Spelled and lemmatized\":                             [spell_tokens, lemmatize_tokens],\n",
    "    \"Spelled and lemmatized (stopword removed)\":          [spell_tokens, lemmatize_tokens, remove_stop_words],\n",
    "    \"Spelled and stemmed\":                                [spell_tokens, stem_tokens],\n",
    "    \"Spelled and stemmed (stopword removed)\":             [spell_tokens, stem_tokens, remove_stop_words],\n",
    "    \"Spelled, stemmed and lemmatized\":                    [spell_tokens, stem_tokens, lemmatize_tokens],\n",
    "    \"Spelled, stemmed and lemmatized (stopword removed)\": [spell_tokens, stem_tokens, lemmatize_tokens, remove_stop_words],\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocessing example"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw tweet (type -1): talking to my over driver about where im goinghe said hed love to go to new york too but since trump its probably not\n",
      "Just tokenized: ['talking', 'to', 'my', 'over', 'driver', 'about', 'where', 'im', 'goinghe', 'said', 'hed', 'love', 'to', 'go', 'to', 'new', 'york', 'too', 'but', 'since', 'trump', 'its', 'probably', 'not']\n",
      "Stemmed: ['talk', 'to', 'my', 'over', 'driver', 'about', 'where', 'im', 'goingh', 'said', 'hed', 'love', 'to', 'go', 'to', 'new', 'york', 'too', 'but', 'sinc', 'trump', 'it', 'probabl', 'not']\n",
      "Stemmed (stopword removed): ['talk', 'driver', 'im', 'goingh', 'said', 'hed', 'love', 'go', 'new', 'york', 'sinc', 'trump', 'probabl']\n",
      "Lemmatized: ['talking', 'to', 'my', 'over', 'driver', 'about', 'where', 'im', 'goinghe', 'said', 'hed', 'love', 'to', 'go', 'to', 'new', 'york', 'too', 'but', 'since', 'trump', 'it', 'probably', 'not']\n",
      "Lemmatized (stopword removed): ['talking', 'driver', 'im', 'goinghe', 'said', 'hed', 'love', 'go', 'new', 'york', 'since', 'trump', 'probably']\n",
      "Spelled: ['talking', 'to', 'my', 'over', 'driver', 'about', 'where', 'im', 'going', 'said', 'hed', 'love', 'to', 'go', 'to', 'new', 'york', 'too', 'but', 'since', 'trump', 'its', 'probably', 'not']\n",
      "Spelled (stopword removed): ['talking', 'driver', 'im', 'going', 'said', 'hed', 'love', 'go', 'new', 'york', 'since', 'trump', 'probably']\n",
      "Spelled and lemmatized: ['talking', 'to', 'my', 'over', 'driver', 'about', 'where', 'im', 'going', 'said', 'hed', 'love', 'to', 'go', 'to', 'new', 'york', 'too', 'but', 'since', 'trump', 'it', 'probably', 'not']\n",
      "Spelled and lemmatized (stopword removed): ['talking', 'driver', 'im', 'going', 'said', 'hed', 'love', 'go', 'new', 'york', 'since', 'trump', 'probably']\n",
      "Spelled and stemmed: ['talk', 'to', 'my', 'over', 'driver', 'about', 'where', 'im', 'go', 'said', 'hed', 'love', 'to', 'go', 'to', 'new', 'york', 'too', 'but', 'sinc', 'trump', 'it', 'probabl', 'not']\n",
      "Spelled and stemmed (stopword removed): ['talk', 'driver', 'im', 'go', 'said', 'hed', 'love', 'go', 'new', 'york', 'sinc', 'trump', 'probabl']\n",
      "Spelled, stemmed and lemmatized: ['talk', 'to', 'my', 'over', 'driver', 'about', 'where', 'im', 'go', 'said', 'hed', 'love', 'to', 'go', 'to', 'new', 'york', 'too', 'but', 'sinc', 'trump', 'it', 'probabl', 'not']\n",
      "Spelled, stemmed and lemmatized (stopword removed): ['talk', 'driver', 'im', 'go', 'said', 'hed', 'love', 'go', 'new', 'york', 'sinc', 'trump', 'probabl']\n"
     ]
    }
   ],
   "source": [
    "tweet = df.iloc[1]\n",
    "print(f'Raw tweet (type {tweet[\"type\"]}): {tweet[\"text\"]}')\n",
    "\n",
    "for name, methods in preprocessing_methods.items():\n",
    "    tokenizer = get_tokenizer(methods)\n",
    "    data = tokenizer(tweet['text'])\n",
    "    print(f'{name}: {data}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data preprocessing ////todo delete"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "preprocessed_data = dict()\n",
    "\n",
    "for name, methods in preprocessing_methods.items():\n",
    "    tokenizer = get_tokenizer(methods)\n",
    "    preprocessed_data[name] = [tokenizer(tweet) for tweet in train_df['text']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 10 similar pairs of tweets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14min 39s, sys: 1.69 s, total: 14min 41s\n",
      "Wall time: 14min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prepared_vectorizers_transforms = dict()\n",
    "\n",
    "for name, methods in preprocessing_methods.items():\n",
    "    tokenizer = get_tokenizer(methods)\n",
    "    vectorizer = TfidfVectorizer(tokenizer=lambda x: tokenizer(x))\n",
    "    prepared_vectorizers_transforms[name] = vectorizer.fit_transform(df['text'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just tokenized\n",
      "\n",
      "0.9549145190462752\n",
      "share the love thanks for being top new followers this week happy want this\n",
      "share the love thanks for being top new followers this week happy want it\n",
      "\n",
      "0.9801880845327963\n",
      "hey thanks for being top new followers this week much appreciated happy want this\n",
      "hey thanks for being my top new followers this week much appreciated happy want this\n",
      "\n",
      "0.9626966357693176\n",
      "and more also in the epaper\n",
      "and more also in epaper\n",
      "\n",
      "0.961753097518891\n",
      "tamil nadu\n",
      "in tamil nadu\n",
      "\n",
      "0.9805362530533835\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "\n",
      "0.9944618246693316\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "\n",
      "0.9801880845327963\n",
      "hey thanks for being my top new followers this week much appreciated happy want this\n",
      "hey thanks for being top new followers this week much appreciated happy want this\n",
      "\n",
      "0.9944618246693316\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "\n",
      "0.9626966357693176\n",
      "and more also in epaper\n",
      "and more also in the epaper\n",
      "\n",
      "0.961753097518891\n",
      "in tamil nadu\n",
      "tamil nadu\n",
      "\n",
      "ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡\n",
      "\n",
      "Stemmed\n",
      "\n",
      "0.961753097518891\n",
      "tamil nadu\n",
      "in tamil nadu\n",
      "\n",
      "0.9626966357693176\n",
      "and more also in epaper\n",
      "and more also in the epaper\n",
      "\n",
      "0.9779831750309851\n",
      "hey thanks for being my top new followers this week much appreciated happy want this\n",
      "hey thanks for being top new followers this week much appreciated happy want this\n",
      "\n",
      "0.9626966357693176\n",
      "and more also in the epaper\n",
      "and more also in epaper\n",
      "\n",
      "0.9779831750309851\n",
      "hey thanks for being top new followers this week much appreciated happy want this\n",
      "hey thanks for being my top new followers this week much appreciated happy want this\n",
      "\n",
      "0.9908282955299917\n",
      "stats for the day have arrived to new followers and no unfollowers happy via\n",
      "stats for the day have arrived new follower and no unfollowers happy via\n",
      "\n",
      "0.9908282955299917\n",
      "stats for the day have arrived new follower and no unfollowers happy via\n",
      "stats for the day have arrived to new followers and no unfollowers happy via\n",
      "\n",
      "0.9939359992201824\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "\n",
      "0.9939359992201824\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "\n",
      "0.978785412768762\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "\n",
      "ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡\n",
      "\n",
      "Stemmed (stopword removed)\n",
      "\n",
      "0.9599463863210744\n",
      "thanks for the recent follow happy to connect happy have a great thursday\n",
      "thanks for the recent follow happy to connect happy have a great thursday want it\n",
      "\n",
      "0.9697125310567251\n",
      "thanks for the recent follow happy to connect happy have a great thursday want it\n",
      "thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "\n",
      "0.9697125310567251\n",
      "thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "thanks for the recent follow happy to connect happy have a great thursday want it\n",
      "\n",
      "0.9697747597987035\n",
      "thanks for the recent follow happy to connect happy have a great thursday get it\n",
      "thanks for the recent follow happy to connect happy have a great thursday get this\n",
      "\n",
      "0.9697747597987035\n",
      "thanks for the recent follow happy to connect happy have a great thursday get this\n",
      "thanks for the recent follow happy to connect happy have a great thursday get it\n",
      "\n",
      "0.9787315127515039\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "\n",
      "0.9734941561829112\n",
      "share the love thanks for being top new followers this week happy want this\n",
      "share the love thanks for being top new followers this week happy want it\n",
      "\n",
      "0.9787315127515039\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "\n",
      "0.9787315127515039\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "\n",
      "0.9734941561829112\n",
      "share the love thanks for being top new followers this week happy want it\n",
      "share the love thanks for being top new followers this week happy want this\n",
      "\n",
      "ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡\n",
      "\n",
      "Lemmatized\n",
      "\n",
      "0.9626966357693176\n",
      "and more also in epaper\n",
      "and more also in the epaper\n",
      "\n",
      "0.9916381579487277\n",
      "stats for the day have arrived new follower and no unfollowers happy via\n",
      "stats for the day have arrived to new followers and no unfollowers happy via\n",
      "\n",
      "0.9999999999999998\n",
      "babies unhappy\n",
      "baby unhappy\n",
      "\n",
      "0.9799738262861636\n",
      "hey thanks for being my top new followers this week much appreciated happy want this\n",
      "hey thanks for being top new followers this week much appreciated happy want this\n",
      "\n",
      "0.9999999999999998\n",
      "baby unhappy\n",
      "babies unhappy\n",
      "\n",
      "0.9941735336111193\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "\n",
      "0.9941735336111193\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "\n",
      "0.9803688174467615\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "\n",
      "0.9799738262861636\n",
      "hey thanks for being top new followers this week much appreciated happy want this\n",
      "hey thanks for being my top new followers this week much appreciated happy want this\n",
      "\n",
      "0.9916381579487277\n",
      "stats for the day have arrived to new followers and no unfollowers happy via\n",
      "stats for the day have arrived new follower and no unfollowers happy via\n",
      "\n",
      "ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡\n",
      "\n",
      "Lemmatized (stopword removed)\n",
      "\n",
      "0.9641937784491623\n",
      "thanks for the recent follow happy to connect happy have a great wednesday\n",
      "thanks for the recent follow happy to connect happy have a great wednesday want this\n",
      "\n",
      "0.9641937784491623\n",
      "thanks for the recent follow happy to connect happy have a great wednesday want this\n",
      "thanks for the recent follow happy to connect happy have a great wednesday\n",
      "\n",
      "0.964242011640493\n",
      "hey thanks for being my top new followers this week much appreciated happy want this\n",
      "hey thanks for being top new followers this week much appreciated happy\n",
      "\n",
      "0.964242011640493\n",
      "hey thanks for being top new followers this week much appreciated happy\n",
      "hey thanks for being top new followers this week much appreciated happy want this\n",
      "\n",
      "0.9806357428852037\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "\n",
      "0.9806357428852037\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "\n",
      "0.9806357428852037\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "\n",
      "0.9999999999999998\n",
      "baby unhappy\n",
      "babies unhappy\n",
      "\n",
      "0.9999999999999998\n",
      "babies unhappy\n",
      "baby unhappy\n",
      "\n",
      "0.964242011640493\n",
      "hey thanks for being top new followers this week much appreciated happy want this\n",
      "hey thanks for being top new followers this week much appreciated happy\n",
      "\n",
      "ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡\n",
      "\n",
      "Spelled\n",
      "\n",
      "0.9546290916474496\n",
      "share the love thanks for being top new followers this week happy want this\n",
      "share the love thanks for being top new followers this week happy want it\n",
      "\n",
      "0.961753097518891\n",
      "tamil nadu\n",
      "in tamil nadu\n",
      "\n",
      "0.9944482514225581\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "\n",
      "0.9806068081157996\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "\n",
      "0.9625611813740301\n",
      "and more also in epaper\n",
      "and more also in the epaper\n",
      "\n",
      "0.9944482514225581\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "\n",
      "0.9625611813740301\n",
      "and more also in the epaper\n",
      "and more also in epaper\n",
      "\n",
      "0.9800807533424065\n",
      "hey thanks for being my top new followers this week much appreciated happy want this\n",
      "hey thanks for being top new followers this week much appreciated happy want this\n",
      "\n",
      "0.9800807533424065\n",
      "hey thanks for being top new followers this week much appreciated happy want this\n",
      "hey thanks for being my top new followers this week much appreciated happy want this\n",
      "\n",
      "0.961753097518891\n",
      "in tamil nadu\n",
      "tamil nadu\n",
      "\n",
      "ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡\n",
      "\n",
      "Spelled (stopword removed)\n",
      "\n",
      "0.9625582819092171\n",
      "thanks for the recent follow happy to connect happy have a great wednesday want this\n",
      "thanks for the recent follow happy to connect happy have a great wednesday\n",
      "\n",
      "0.9628739900932984\n",
      "hey thanks for being top new followers this week much appreciated happy\n",
      "hey thanks for being top new followers this week much appreciated happy want this\n",
      "\n",
      "0.9628739900932984\n",
      "hey thanks for being top new followers this week much appreciated happy want this\n",
      "hey thanks for being top new followers this week much appreciated happy\n",
      "\n",
      "0.9628739900932984\n",
      "hey thanks for being my top new followers this week much appreciated happy want this\n",
      "hey thanks for being top new followers this week much appreciated happy\n",
      "\n",
      "0.9809238489747437\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "\n",
      "0.9999999999999996\n",
      "thanks for the recent follow much appreciated happy get it\n",
      "thanks for the recent follow much appreciated happy get this\n",
      "\n",
      "0.9809238489747437\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "\n",
      "0.9809238489747437\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "\n",
      "0.9625582819092171\n",
      "thanks for the recent follow happy to connect happy have a great wednesday\n",
      "thanks for the recent follow happy to connect happy have a great wednesday want this\n",
      "\n",
      "0.9999999999999996\n",
      "thanks for the recent follow much appreciated happy get this\n",
      "thanks for the recent follow much appreciated happy get it\n",
      "\n",
      "ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡\n",
      "\n",
      "Spelled and lemmatized\n",
      "\n",
      "0.961753097518891\n",
      "tamil nadu\n",
      "in tamil nadu\n",
      "\n",
      "0.9625611813740301\n",
      "and more also in epaper\n",
      "and more also in the epaper\n",
      "\n",
      "0.9625611813740301\n",
      "and more also in the epaper\n",
      "and more also in epaper\n",
      "\n",
      "0.9799318028473405\n",
      "hey thanks for being my top new followers this week much appreciated happy want this\n",
      "hey thanks for being top new followers this week much appreciated happy want this\n",
      "\n",
      "0.9941575839718166\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "\n",
      "0.9804220881788763\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "\n",
      "0.9941575839718166\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "\n",
      "0.9799318028473405\n",
      "hey thanks for being top new followers this week much appreciated happy want this\n",
      "hey thanks for being my top new followers this week much appreciated happy want this\n",
      "\n",
      "0.9926122881131061\n",
      "stats for the day have arrived to new followers and no unfollowers happy via\n",
      "stats for the day have arrived new follower and no unfollowers happy via\n",
      "\n",
      "0.9926122881131061\n",
      "stats for the day have arrived new follower and no unfollowers happy via\n",
      "stats for the day have arrived to new followers and no unfollowers happy via\n",
      "\n",
      "ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡\n",
      "\n",
      "Spelled and lemmatized (stopword removed)\n",
      "\n",
      "0.9614506765773134\n",
      "thanks for the recent follow happy to connect happy have a great thursday want it\n",
      "thanks for the recent follow happy to connect happy have a great thursday\n",
      "\n",
      "0.9614506765773134\n",
      "thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "thanks for the recent follow happy to connect happy have a great thursday\n",
      "\n",
      "0.9643811423130969\n",
      "thanks for the recent follow happy to connect happy have a great wednesday\n",
      "thanks for the recent follow happy to connect happy have a great wednesday want this\n",
      "\n",
      "0.9643811423130969\n",
      "thanks for the recent follow happy to connect happy have a great wednesday want this\n",
      "thanks for the recent follow happy to connect happy have a great wednesday\n",
      "\n",
      "0.9644408022758854\n",
      "hey thanks for being my top new followers this week much appreciated happy want this\n",
      "hey thanks for being top new followers this week much appreciated happy\n",
      "\n",
      "0.9644408022758854\n",
      "hey thanks for being top new followers this week much appreciated happy\n",
      "hey thanks for being top new followers this week much appreciated happy want this\n",
      "\n",
      "0.9644408022758854\n",
      "hey thanks for being top new followers this week much appreciated happy want this\n",
      "hey thanks for being top new followers this week much appreciated happy\n",
      "\n",
      "0.9807142922446532\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "\n",
      "0.9807142922446532\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "\n",
      "0.9807142922446532\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "\n",
      "ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡\n",
      "\n",
      "Spelled and stemmed\n",
      "\n",
      "0.961753097518891\n",
      "in tamil nadu\n",
      "tamil nadu\n",
      "\n",
      "0.9625611813740301\n",
      "and more also in the epaper\n",
      "and more also in epaper\n",
      "\n",
      "0.9625611813740301\n",
      "and more also in epaper\n",
      "and more also in the epaper\n",
      "\n",
      "0.9779059574028904\n",
      "hey thanks for being top new followers this week much appreciated happy want this\n",
      "hey thanks for being my top new followers this week much appreciated happy want this\n",
      "\n",
      "0.978806814962104\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "\n",
      "0.9779059574028904\n",
      "hey thanks for being my top new followers this week much appreciated happy want this\n",
      "hey thanks for being top new followers this week much appreciated happy want this\n",
      "\n",
      "0.9912332154843165\n",
      "stats for the day have arrived new follower and no unfollowers happy via\n",
      "stats for the day have arrived to new followers and no unfollowers happy via\n",
      "\n",
      "0.9939043175454587\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "\n",
      "0.9912332154843165\n",
      "stats for the day have arrived to new followers and no unfollowers happy via\n",
      "stats for the day have arrived new follower and no unfollowers happy via\n",
      "\n",
      "0.9939043175454587\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "\n",
      "ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡\n",
      "\n",
      "Spelled and stemmed (stopword removed)\n",
      "\n",
      "0.9600914449136776\n",
      "thanks for the recent follow happy to connect happy have a great thursday\n",
      "thanks for the recent follow happy to connect happy have a great thursday want it\n",
      "\n",
      "0.9787744547457674\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "\n",
      "0.9696762295006242\n",
      "thanks for the recent follow happy to connect happy have a great thursday want it\n",
      "thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "\n",
      "0.9787744547457674\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "\n",
      "0.9733147867396652\n",
      "share the love thanks for being top new followers this week happy want this\n",
      "share the love thanks for being top new followers this week happy want it\n",
      "\n",
      "0.9733147867396652\n",
      "share the love thanks for being top new followers this week happy want it\n",
      "share the love thanks for being top new followers this week happy want this\n",
      "\n",
      "0.9787744547457674\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "\n",
      "0.9696762295006242\n",
      "thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "thanks for the recent follow happy to connect happy have a great thursday want it\n",
      "\n",
      "0.9697466039287853\n",
      "thanks for the recent follow happy to connect happy have a great thursday get this\n",
      "thanks for the recent follow happy to connect happy have a great thursday get it\n",
      "\n",
      "0.9697466039287853\n",
      "thanks for the recent follow happy to connect happy have a great thursday get it\n",
      "thanks for the recent follow happy to connect happy have a great thursday get this\n",
      "\n",
      "ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡\n",
      "\n",
      "Spelled, stemmed and lemmatized\n",
      "\n",
      "0.961753097518891\n",
      "in tamil nadu\n",
      "tamil nadu\n",
      "\n",
      "0.9625611813740301\n",
      "and more also in the epaper\n",
      "and more also in epaper\n",
      "\n",
      "0.9625611813740301\n",
      "and more also in epaper\n",
      "and more also in the epaper\n",
      "\n",
      "0.9779059574028904\n",
      "hey thanks for being top new followers this week much appreciated happy want this\n",
      "hey thanks for being my top new followers this week much appreciated happy want this\n",
      "\n",
      "0.9779059574028904\n",
      "hey thanks for being my top new followers this week much appreciated happy want this\n",
      "hey thanks for being top new followers this week much appreciated happy want this\n",
      "\n",
      "0.993889617520387\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "\n",
      "0.9912332154843165\n",
      "stats for the day have arrived to new followers and no unfollowers happy via\n",
      "stats for the day have arrived new follower and no unfollowers happy via\n",
      "\n",
      "0.9912332154843165\n",
      "stats for the day have arrived new follower and no unfollowers happy via\n",
      "stats for the day have arrived to new followers and no unfollowers happy via\n",
      "\n",
      "0.993889617520387\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "\n",
      "0.9795705337159327\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "\n",
      "ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡\n",
      "\n",
      "Spelled, stemmed and lemmatized (stopword removed)\n",
      "\n",
      "0.9600914449136776\n",
      "thanks for the recent follow happy to connect happy have a great thursday\n",
      "thanks for the recent follow happy to connect happy have a great thursday want it\n",
      "\n",
      "0.9696762295006242\n",
      "thanks for the recent follow happy to connect happy have a great thursday want it\n",
      "thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "\n",
      "0.9697466039287853\n",
      "thanks for the recent follow happy to connect happy have a great thursday get this\n",
      "thanks for the recent follow happy to connect happy have a great thursday get it\n",
      "\n",
      "0.9696762295006242\n",
      "thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "thanks for the recent follow happy to connect happy have a great thursday want it\n",
      "\n",
      "0.9697466039287853\n",
      "thanks for the recent follow happy to connect happy have a great thursday get it\n",
      "thanks for the recent follow happy to connect happy have a great thursday get this\n",
      "\n",
      "0.9797217891276386\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "\n",
      "0.9733147867396652\n",
      "share the love thanks for being top new followers this week happy want this\n",
      "share the love thanks for being top new followers this week happy want it\n",
      "\n",
      "0.9797217891276386\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "\n",
      "0.9733147867396652\n",
      "share the love thanks for being top new followers this week happy want it\n",
      "share the love thanks for being top new followers this week happy want this\n",
      "\n",
      "0.9797217891276386\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "\n",
      "ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡\n",
      "\n",
      "CPU times: user 1.82 s, sys: 188 ms, total: 2.01 s\n",
      "Wall time: 1.92 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def find_top_similarity_tweets(similarity_tweets_matrix):\n",
    "    N = 10\n",
    "    i = 0\n",
    "\n",
    "    similarity_index_array = np.zeros(shape=(len(similarity_tweets_matrix)), dtype=np.int16)\n",
    "    similarity_value_array = np.zeros(shape=(len(similarity_tweets_matrix)))\n",
    "\n",
    "    while i < len(similarity_tweets_matrix):\n",
    "        array = np.array(similarity_tweets_matrix[i])\n",
    "        array[i] = 0.0\n",
    "        array[array >= 1.0] = 0.0\n",
    "        max_index = np.argmax(array)\n",
    "        similarity_index_array[i] = max_index\n",
    "        similarity_value_array[i] = array[max_index]\n",
    "        i += 1\n",
    "\n",
    "    top_indexes = np.argpartition(similarity_value_array, -N)[-N:]\n",
    "\n",
    "    for index in top_indexes:\n",
    "        print(similarity_value_array[index])\n",
    "        print(df.iloc[index]['text'])\n",
    "        print(df.iloc[similarity_index_array[index]]['text'])\n",
    "        print('')\n",
    "\n",
    "\n",
    "for name, transform in prepared_vectorizers_transforms.items():\n",
    "    print(f'{name}\\n')\n",
    "    similarity_matrix = cosine_similarity(transform)\n",
    "    find_top_similarity_tweets(similarity_matrix)\n",
    "    print(f'{OUTPUT_SEPARATOR}\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Choose best preprocessing method"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "def choose_best_preprocessing_method(methods_dict):\n",
    "    best_method_name = ''\n",
    "    best_vectorizer = ''\n",
    "    best_accuracy = 0\n",
    "    for name, methods in methods_dict.items():\n",
    "        tokenizer = get_tokenizer(methods)\n",
    "\n",
    "        bin_vectorizer = CountVectorizer(tokenizer=lambda x: tokenizer(x), binary=True)\n",
    "        count_vectorizer = CountVectorizer(tokenizer=lambda x: tokenizer(x))\n",
    "        tfidf_vectorizer = TfidfVectorizer(tokenizer=lambda x: tokenizer(x))\n",
    "\n",
    "        model = RandomForestClassifier(\n",
    "            class_weight='balanced',\n",
    "            criterion='entropy',\n",
    "            max_depth=170,\n",
    "            n_estimators=200,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        grid_pipline_bin_vec = Pipeline([\n",
    "            ('vectorizer', bin_vectorizer),\n",
    "            ('model', model),\n",
    "        ])\n",
    "\n",
    "        grid_pipline_count_vec = Pipeline([\n",
    "            ('vectorizer', count_vectorizer),\n",
    "            ('model', model),\n",
    "        ])\n",
    "\n",
    "        grid_pipline_tfidf_vec = Pipeline([\n",
    "            ('vectorizer', tfidf_vectorizer),\n",
    "            ('model', model),\n",
    "        ])\n",
    "\n",
    "        grid_pipline_bin_vec.fit(train_df['text'], train_df['type'])\n",
    "        grid_pipline_count_vec.fit(train_df['text'], train_df['type'])\n",
    "        grid_pipline_tfidf_vec.fit(train_df['text'], train_df['type'])\n",
    "\n",
    "        y_pred_bin_vect = grid_pipline_bin_vec.predict(test_df['text'])\n",
    "        y_pred_count_vect = grid_pipline_count_vec.predict(test_df['text'])\n",
    "        y_pred_tfidf_vect = grid_pipline_tfidf_vec.predict(test_df['text'])\n",
    "        y_true = test_df['type']\n",
    "\n",
    "        accuracy_bin_vec = accuracy_score(y_true, y_pred_bin_vect)\n",
    "        accuracy_count_vec = accuracy_score(y_true, y_pred_count_vect)\n",
    "        accuracy_tfidf_vec = accuracy_score(y_true, y_pred_tfidf_vect)\n",
    "\n",
    "        current_best_accuracy = 0\n",
    "        current_best_vectorizer = ''\n",
    "        if accuracy_bin_vec > current_best_accuracy:\n",
    "            current_best_accuracy = accuracy_bin_vec\n",
    "            current_best_vectorizer = 'bin vectorizer'\n",
    "        elif accuracy_count_vec > current_best_accuracy:\n",
    "            current_best_accuracy = accuracy_count_vec\n",
    "            current_best_vectorizer = 'count vectorizer'\n",
    "        elif accuracy_tfidf_vec > current_best_accuracy:\n",
    "            current_best_accuracy = accuracy_tfidf_vec\n",
    "            current_best_vectorizer = 'tfidf vectorizer'\n",
    "\n",
    "        print(f'Preprocessing methods: {name}')\n",
    "        print(f'accuracy bin vectorizer: {accuracy_bin_vec}')\n",
    "        print(f'accuracy count vectorizer: {accuracy_count_vec}')\n",
    "        print(f'accuracy tfidf vectorizer: {accuracy_tfidf_vec}\\n')\n",
    "\n",
    "        if current_best_accuracy > best_accuracy:\n",
    "            best_accuracy = current_best_accuracy\n",
    "            best_method_name = name\n",
    "            best_vectorizer = current_best_vectorizer\n",
    "\n",
    "    return {'name': best_method_name, 'vectorizer': best_vectorizer, 'accuracy': best_accuracy}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing methods: Just tokenized\n",
      "accuracy bin vectorizer: 0.8851612903225806\n",
      "accuracy count vectorizer: 0.8851612903225806\n",
      "accuracy tfidf vectorizer: 0.8941935483870967\n",
      "\n",
      "Preprocessing methods: Stemmed\n",
      "accuracy bin vectorizer: 0.8903225806451613\n",
      "accuracy count vectorizer: 0.8903225806451613\n",
      "accuracy tfidf vectorizer: 0.895483870967742\n",
      "\n",
      "Preprocessing methods: Stemmed (stopword removed)\n",
      "accuracy bin vectorizer: 0.8761290322580645\n",
      "accuracy count vectorizer: 0.8761290322580645\n",
      "accuracy tfidf vectorizer: 0.8877419354838709\n",
      "\n",
      "Preprocessing methods: Lemmatized\n",
      "accuracy bin vectorizer: 0.8890322580645161\n",
      "accuracy count vectorizer: 0.8890322580645161\n",
      "accuracy tfidf vectorizer: 0.8941935483870967\n",
      "\n",
      "Preprocessing methods: Lemmatized (stopword removed)\n",
      "accuracy bin vectorizer: 0.8838709677419355\n",
      "accuracy count vectorizer: 0.8838709677419355\n",
      "accuracy tfidf vectorizer: 0.8877419354838709\n",
      "\n",
      "Preprocessing methods: Spelled\n",
      "accuracy bin vectorizer: 0.8916129032258064\n",
      "accuracy count vectorizer: 0.8916129032258064\n",
      "accuracy tfidf vectorizer: 0.8993548387096775\n",
      "\n",
      "Preprocessing methods: Spelled (stopword removed)\n",
      "accuracy bin vectorizer: 0.8851612903225806\n",
      "accuracy count vectorizer: 0.8851612903225806\n",
      "accuracy tfidf vectorizer: 0.8903225806451613\n",
      "\n",
      "Preprocessing methods: Spelled and lemmatized\n",
      "accuracy bin vectorizer: 0.8903225806451613\n",
      "accuracy count vectorizer: 0.8903225806451613\n",
      "accuracy tfidf vectorizer: 0.8980645161290323\n",
      "\n",
      "Preprocessing methods: Spelled and lemmatized (stopword removed)\n",
      "accuracy bin vectorizer: 0.8825806451612903\n",
      "accuracy count vectorizer: 0.8825806451612903\n",
      "accuracy tfidf vectorizer: 0.8916129032258064\n",
      "\n",
      "Preprocessing methods: Spelled and stemmed\n",
      "accuracy bin vectorizer: 0.8993548387096775\n",
      "accuracy count vectorizer: 0.8993548387096775\n",
      "accuracy tfidf vectorizer: 0.8993548387096775\n",
      "\n",
      "Preprocessing methods: Spelled and stemmed (stopword removed)\n",
      "accuracy bin vectorizer: 0.8812903225806452\n",
      "accuracy count vectorizer: 0.8812903225806452\n",
      "accuracy tfidf vectorizer: 0.8877419354838709\n",
      "\n",
      "Preprocessing methods: Spelled, stemmed and lemmatized\n",
      "accuracy bin vectorizer: 0.896774193548387\n",
      "accuracy count vectorizer: 0.896774193548387\n",
      "accuracy tfidf vectorizer: 0.9019354838709678\n",
      "\n",
      "Preprocessing methods: Spelled, stemmed and lemmatized (stopword removed)\n",
      "accuracy bin vectorizer: 0.8787096774193548\n",
      "accuracy count vectorizer: 0.8787096774193548\n",
      "accuracy tfidf vectorizer: 0.8903225806451613\n",
      "\n",
      "Best preprocessing method: {'name': 'Spelled and stemmed', 'vectorizer': 'bin vectorizer', 'accuracy': 0.8993548387096775}\n",
      "ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡\n",
      "CPU times: user 34min 9s, sys: 3.93 s, total: 34min 13s\n",
      "Wall time: 34min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_method = choose_best_preprocessing_method(preprocessing_methods)\n",
    "\n",
    "print(f'Best preprocessing method: {best_method}')\n",
    "print(OUTPUT_SEPARATOR)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get best params"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 380 µs, sys: 2 µs, total: 382 µs\n",
      "Wall time: 389 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [1, 10, 30, 50, 70, 100, 130, 150, 170, 190, 230],\n",
    "    'n_estimators': (5, 10, 20, 40, 60, 100, 150, 200, 300),\n",
    "    'criterion': ('entropy', 'gini'),\n",
    "    'class_weight': ['balanced', None]\n",
    "}\n",
    "\n",
    "methods = preprocessing_methods['Spelled and stemmed']\n",
    "tokenizer = get_tokenizer(methods)\n",
    "\n",
    "bin_vectorizer = CountVectorizer(tokenizer=lambda x: tokenizer(x), binary=True)\n",
    "count_vectorizer = CountVectorizer(tokenizer=lambda x: tokenizer(x))\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=lambda x: tokenizer(x))\n",
    "\n",
    "grid_pipline_model = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=0),\n",
    "    param_grid=param_grid,\n",
    "    cv=4,\n",
    "    verbose=2,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_pipline_bin_vectorizer = Pipeline([\n",
    "    ('vectorizer', bin_vectorizer),\n",
    "    ('model', grid_pipline_model),\n",
    "])\n",
    "\n",
    "grid_pipline_count_vectorizer = Pipeline([\n",
    "    ('vectorizer', count_vectorizer),\n",
    "    ('model', grid_pipline_model),\n",
    "])\n",
    "\n",
    "grid_pipline_tfidf_vectorizer = Pipeline([\n",
    "    ('vectorizer', tfidf_vectorizer),\n",
    "    ('model', grid_pipline_model),\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 396 candidates, totalling 1584 fits\n",
      "Fitting 4 folds for each of 396 candidates, totalling 1584 fits\n",
      "Fitting 4 folds for each of 396 candidates, totalling 1584 fits\n",
      "Bin vectorizer:\n",
      "best params: {'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 190, 'n_estimators': 200}\n",
      "accuracy: 0.9045161290322581\n",
      "Count vectorizer:\n",
      "best params: {'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 190, 'n_estimators': 200}\n",
      "accuracy: 0.9045161290322581\n",
      "TFIDF vectorizer:\n",
      "best params: {'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 190, 'n_estimators': 200}\n",
      "accuracy: 0.9006451612903226\n",
      "ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡\n",
      "CPU times: user 7min 26s, sys: 4.42 s, total: 7min 31s\n",
      "Wall time: 33min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "grid_pipline_bin_vectorizer.fit(train_df['text'], train_df['type'])\n",
    "grid_pipline_count_vectorizer.fit(train_df['text'], train_df['type'])\n",
    "grid_pipline_tfidf_vectorizer.fit(train_df['text'], train_df['type'])\n",
    "\n",
    "best_params_bin_vectorizer = grid_pipline_bin_vectorizer['model'].best_params_\n",
    "best_params_count_vectorizer = grid_pipline_count_vectorizer['model'].best_params_\n",
    "best_params_tfidf_vectorizer = grid_pipline_tfidf_vectorizer['model'].best_params_\n",
    "\n",
    "y_pred_bin_vectorizer = grid_pipline_bin_vectorizer.predict(test_df['text'])\n",
    "y_pred_count_vectorizer = grid_pipline_count_vectorizer.predict(test_df['text'])\n",
    "y_pred_tfidf_vectorizer = grid_pipline_tfidf_vectorizer.predict(test_df['text'])\n",
    "y_true = test_df['type']\n",
    "\n",
    "accuracy_bin_vectorizer = accuracy_score(y_true, y_pred_bin_vectorizer)\n",
    "accuracy_count_vectorizer = accuracy_score(y_true, y_pred_count_vectorizer)\n",
    "accuracy_tfidf_vectorizer = accuracy_score(y_true, y_pred_tfidf_vectorizer)\n",
    "\n",
    "print('Bin vectorizer:')\n",
    "print(f'best params: {best_params_bin_vectorizer}')\n",
    "print(f'accuracy: {accuracy_bin_vectorizer}')\n",
    "print('Count vectorizer:')\n",
    "print(f'best params: {best_params_count_vectorizer}')\n",
    "print(f'accuracy: {accuracy_count_vectorizer}')\n",
    "print('TFIDF vectorizer:')\n",
    "print(f'best params: {best_params_tfidf_vectorizer}')\n",
    "print(f'accuracy: {accuracy_tfidf_vectorizer}')\n",
    "print(OUTPUT_SEPARATOR)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 225 µs, sys: 10 µs, total: 235 µs\n",
      "Wall time: 241 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': list(range(180, 200)),\n",
    "    'n_estimators': list(range(190, 210)),\n",
    "}\n",
    "\n",
    "bin_vectorizer = CountVectorizer(tokenizer=lambda x: tokenizer(x), binary=True)\n",
    "count_vectorizer = CountVectorizer(tokenizer=lambda x: tokenizer(x))\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=lambda x: tokenizer(x))\n",
    "\n",
    "grid_pipline_model = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=0, class_weight='balanced'),\n",
    "    param_grid=param_grid,\n",
    "    cv=4,\n",
    "    verbose=2,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_pipline_bin_vectorizer = Pipeline([\n",
    "    ('vectorizer', bin_vectorizer),\n",
    "    ('model', grid_pipline_model),\n",
    "])\n",
    "\n",
    "grid_pipline_count_vectorizer = Pipeline([\n",
    "    ('vectorizer', count_vectorizer),\n",
    "    ('model', grid_pipline_model),\n",
    "])\n",
    "\n",
    "grid_pipline_tfidf_vectorizer = Pipeline([\n",
    "    ('vectorizer', tfidf_vectorizer),\n",
    "    ('model', grid_pipline_model),\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 400 candidates, totalling 1600 fits\n",
      "Fitting 4 folds for each of 400 candidates, totalling 1600 fits\n",
      "Fitting 4 folds for each of 400 candidates, totalling 1600 fits\n",
      "Bin vectorizer:\n",
      "best params: {'max_depth': 189, 'n_estimators': 195}\n",
      "accuracy: 0.9006451612903226\n",
      "Count vectorizer:\n",
      "best params: {'max_depth': 189, 'n_estimators': 195}\n",
      "accuracy: 0.9006451612903226\n",
      "TFIDF vectorizer:\n",
      "best params: {'max_depth': 189, 'n_estimators': 195}\n",
      "accuracy: 0.9019354838709678\n",
      "ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡\n",
      "CPU times: user 6min 58s, sys: 4.34 s, total: 7min 2s\n",
      "Wall time: 1h 6min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "grid_pipline_bin_vectorizer.fit(train_df['text'], train_df['type'])\n",
    "grid_pipline_count_vectorizer.fit(train_df['text'], train_df['type'])\n",
    "grid_pipline_tfidf_vectorizer.fit(train_df['text'], train_df['type'])\n",
    "\n",
    "best_params_bin_vectorizer = grid_pipline_bin_vectorizer['model'].best_params_\n",
    "best_params_count_vectorizer = grid_pipline_count_vectorizer['model'].best_params_\n",
    "best_params_tfidf_vectorizer = grid_pipline_tfidf_vectorizer['model'].best_params_\n",
    "\n",
    "y_pred_bin_vectorizer = grid_pipline_bin_vectorizer.predict(test_df['text'])\n",
    "y_pred_count_vectorizer = grid_pipline_count_vectorizer.predict(test_df['text'])\n",
    "y_pred_tfidf_vectorizer = grid_pipline_tfidf_vectorizer.predict(test_df['text'])\n",
    "y_true = test_df['type']\n",
    "\n",
    "accuracy_bin_vectorizer = accuracy_score(y_true, y_pred_bin_vectorizer)\n",
    "accuracy_count_vectorizer = accuracy_score(y_true, y_pred_count_vectorizer)\n",
    "accuracy_tfidf_vectorizer = accuracy_score(y_true, y_pred_tfidf_vectorizer)\n",
    "\n",
    "print('Bin vectorizer:')\n",
    "print(f'best params: {best_params_bin_vectorizer}')\n",
    "print(f'accuracy: {accuracy_bin_vectorizer}')\n",
    "print('Count vectorizer:')\n",
    "print(f'best params: {best_params_count_vectorizer}')\n",
    "print(f'accuracy: {accuracy_count_vectorizer}')\n",
    "print('TFIDF vectorizer:')\n",
    "print(f'best params: {best_params_tfidf_vectorizer}')\n",
    "print(f'accuracy: {accuracy_tfidf_vectorizer}')\n",
    "print(OUTPUT_SEPARATOR)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Different classification approaches"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [],
   "source": [
    "methods = preprocessing_methods['Spelled and stemmed']\n",
    "tokenizer = get_tokenizer(methods)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Random forest with best params"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 50s, sys: 710 ms, total: 2min 51s\n",
      "Wall time: 2min 52s\n"
     ]
    },
    {
     "data": {
      "text/plain": "Pipeline(steps=[('vectorizer',\n                 TfidfVectorizer(tokenizer=<function <lambda> at 0x7fe96c9415e0>)),\n                ('model',\n                 RandomForestClassifier(class_weight='balanced', max_depth=189,\n                                        n_estimators=195, random_state=0))])"
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=lambda x: tokenizer(x))\n",
    "model = RandomForestClassifier(random_state=0, class_weight='balanced', max_depth=189, n_estimators=195)\n",
    "\n",
    "pipline = Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('model', model),\n",
    "])\n",
    "\n",
    "pipline.fit(train_df['text'], train_df['type'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8801169590643275\n",
      "CPU times: user 27.4 s, sys: 46.5 ms, total: 27.4 s\n",
      "Wall time: 27.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "y_true = test_df['type']\n",
    "y_pred = pipline.predict(test_df['text'])\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "print(f'accuracy: {accuracy}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### CatBoost"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 30s, sys: 5.69 s, total: 4min 35s\n",
      "Wall time: 1min 37s\n"
     ]
    },
    {
     "data": {
      "text/plain": "Pipeline(steps=[('vectorizer',\n                 TfidfVectorizer(tokenizer=<function <lambda> at 0x7fe955b604c0>)),\n                ('model',\n                 <catboost.core.CatBoostClassifier object at 0x7fe978583d30>)])"
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=lambda x: tokenizer(x))\n",
    "model = CatBoostClassifier()\n",
    "\n",
    "pipline = Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('model', model),\n",
    "])\n",
    "\n",
    "pipline.fit(train_df['text'], train_df['type'], model__silent=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8611111111111112\n",
      "CPU times: user 27.1 s, sys: 43.6 ms, total: 27.1 s\n",
      "Wall time: 27.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "y_true = test_df['type']\n",
    "y_pred = pipline.predict(test_df['text'])\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "print(f'accuracy: {accuracy}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xtr, ytr, xte, yte = train_test_split(w2c)\n",
    "model = RandomForestClassifier(**best_params)\n",
    "model.fit(xtr, ytr)\n",
    "\n",
    "print(\"accuracy:\", accuracy_score(model.predict(xte), yte))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = CatBoostClassifier()\n",
    "model.fit(xtr, ytr, silent=True)\n",
    "\n",
    "print(\"accuracy:\", accuracy_score(model.predict(xte), yte))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}