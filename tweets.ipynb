{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tweets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import re\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data preparation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative_df shape: (1117, 1)\n",
      "neutral_df shape: (1570, 1)\n",
      "positive_df shape: (1186, 1)\n",
      "df shape: (3873, 2)\n",
      "count types\n",
      " 0    1570\n",
      " 1    1186\n",
      "-1    1117\n",
      "Name: type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_SEPARATOR = 'ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡'\n",
    "\n",
    "TYPE_NEGATIVE = -1\n",
    "TYPE_NEUTRAL = 0\n",
    "TYPE_POSITIVE = 1\n",
    "\n",
    "negative_df = pd.DataFrame(pd.read_csv('./data/processedNegative.csv').columns, columns=['text'])\n",
    "neutral_df = pd.DataFrame(pd.read_csv('./data/processedNeutral.csv').columns, columns=['text'])\n",
    "positive_df = pd.DataFrame(pd.read_csv('./data/processedPositive.csv').columns, columns=['text'])\n",
    "\n",
    "print(f'negative_df shape: {negative_df.shape}')\n",
    "print(f'neutral_df shape: {neutral_df.shape}')\n",
    "print(f'positive_df shape: {positive_df.shape}')\n",
    "\n",
    "negative_df['type'] = TYPE_NEGATIVE\n",
    "neutral_df['type'] = TYPE_NEUTRAL\n",
    "positive_df['type'] = TYPE_POSITIVE\n",
    "df = pd.concat([negative_df, positive_df, neutral_df])\n",
    "\n",
    "print(f'df shape: {df.shape}')\n",
    "print('count types')\n",
    "print(df['type'].value_counts())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Drop duplicates"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "outputs": [],
   "source": [
    "def drop_duplications_preparation(tweet):\n",
    "    tweet_wituot_digits = re.sub(r\"\\d+\", '', tweet.lower())\n",
    "    tweet_wituot_digits_and_punctuation = re.sub(r'[^\\w\\s]', '', tweet_wituot_digits)\n",
    "    tweet_wituot_digits_and_punctuation_and_add_spaces = re.sub(' +', ' ', tweet_wituot_digits_and_punctuation)\n",
    "    return tweet_wituot_digits_and_punctuation_and_add_spaces.strip()\n",
    "\n",
    "\n",
    "df['text'] = df['text'].apply(drop_duplications_preparation)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df shape: (3417, 2)\n",
      "count types\n",
      " 0    1470\n",
      "-1     974\n",
      " 1     973\n",
      "Name: type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = df.drop_duplicates()\n",
    "\n",
    "print(f'df shape: {df.shape}')\n",
    "print('count types')\n",
    "print(df['type'].value_counts())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df count types\n",
      "(2733, 2)\n",
      " 0    1181\n",
      " 1     780\n",
      "-1     772\n",
      "Name: type, dtype: int64\n",
      "test_df count types\n",
      "(684, 2)\n",
      " 0    289\n",
      "-1    202\n",
      " 1    193\n",
      "Name: type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2)\n",
    "\n",
    "print('train_df count types')\n",
    "print(train_df.shape)\n",
    "print(train_df['type'].value_counts())\n",
    "print('test_df count types')\n",
    "print(test_df.shape)\n",
    "print(test_df['type'].value_counts())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Crete preprocessing methods"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18 µs, sys: 1 µs, total: 19 µs\n",
      "Wall time: 24.1 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from tokenizer import remove_stop_words, stem_tokens, lemmatize_tokens, spell_tokens, get_tokenizer\n",
    "\n",
    "preprocessing_methods = {\n",
    "    \"Just tokenized\":                                     [],\n",
    "    \"Stemmed\":                                            [stem_tokens],\n",
    "    \"Stemmed (stopword removed)\":                         [stem_tokens, remove_stop_words],\n",
    "    \"Lemmatized\":                                         [lemmatize_tokens],\n",
    "    \"Lemmatized (stopword removed)\":                      [lemmatize_tokens, remove_stop_words],\n",
    "    \"Spelled\":                                            [spell_tokens],\n",
    "    \"Spelled (stopword removed)\":                         [spell_tokens, remove_stop_words],\n",
    "    \"Spelled and lemmatized\":                             [spell_tokens, lemmatize_tokens],\n",
    "    \"Spelled and lemmatized (stopword removed)\":          [spell_tokens, lemmatize_tokens, remove_stop_words],\n",
    "    \"Spelled and stemmed\":                                [spell_tokens, stem_tokens],\n",
    "    \"Spelled and stemmed (stopword removed)\":             [spell_tokens, stem_tokens, remove_stop_words],\n",
    "    \"Spelled, stemmed and lemmatized\":                    [spell_tokens, stem_tokens, lemmatize_tokens],\n",
    "    \"Spelled, stemmed and lemmatized (stopword removed)\": [spell_tokens, stem_tokens, lemmatize_tokens, remove_stop_words],\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocessing example"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw tweet (type -1): talking to my over driver about where im goinghe said hed love to go to new york too but since trump its probably not\n",
      "Just tokenized: ['talking', 'to', 'my', 'over', 'driver', 'about', 'where', 'im', 'goinghe', 'said', 'hed', 'love', 'to', 'go', 'to', 'new', 'york', 'too', 'but', 'since', 'trump', 'its', 'probably', 'not']\n",
      "Stemmed: ['talk', 'to', 'my', 'over', 'driver', 'about', 'where', 'im', 'goingh', 'said', 'hed', 'love', 'to', 'go', 'to', 'new', 'york', 'too', 'but', 'sinc', 'trump', 'it', 'probabl', 'not']\n",
      "Stemmed (stopword removed): ['talk', 'driver', 'im', 'goingh', 'said', 'hed', 'love', 'go', 'new', 'york', 'sinc', 'trump', 'probabl']\n",
      "Lemmatized: ['talking', 'to', 'my', 'over', 'driver', 'about', 'where', 'im', 'goinghe', 'said', 'hed', 'love', 'to', 'go', 'to', 'new', 'york', 'too', 'but', 'since', 'trump', 'it', 'probably', 'not']\n",
      "Lemmatized (stopword removed): ['talking', 'driver', 'im', 'goinghe', 'said', 'hed', 'love', 'go', 'new', 'york', 'since', 'trump', 'probably']\n",
      "Spelled: ['talking', 'to', 'my', 'over', 'driver', 'about', 'where', 'im', 'going', 'said', 'hed', 'love', 'to', 'go', 'to', 'new', 'york', 'too', 'but', 'since', 'trump', 'its', 'probably', 'not']\n",
      "Spelled (stopword removed): ['talking', 'driver', 'im', 'going', 'said', 'hed', 'love', 'go', 'new', 'york', 'since', 'trump', 'probably']\n",
      "Spelled and lemmatized: ['talking', 'to', 'my', 'over', 'driver', 'about', 'where', 'im', 'going', 'said', 'hed', 'love', 'to', 'go', 'to', 'new', 'york', 'too', 'but', 'since', 'trump', 'it', 'probably', 'not']\n",
      "Spelled and lemmatized (stopword removed): ['talking', 'driver', 'im', 'going', 'said', 'hed', 'love', 'go', 'new', 'york', 'since', 'trump', 'probably']\n",
      "Spelled and stemmed: ['talk', 'to', 'my', 'over', 'driver', 'about', 'where', 'im', 'go', 'said', 'hed', 'love', 'to', 'go', 'to', 'new', 'york', 'too', 'but', 'sinc', 'trump', 'it', 'probabl', 'not']\n",
      "Spelled and stemmed (stopword removed): ['talk', 'driver', 'im', 'go', 'said', 'hed', 'love', 'go', 'new', 'york', 'sinc', 'trump', 'probabl']\n",
      "Spelled, stemmed and lemmatized: ['talk', 'to', 'my', 'over', 'driver', 'about', 'where', 'im', 'go', 'said', 'hed', 'love', 'to', 'go', 'to', 'new', 'york', 'too', 'but', 'sinc', 'trump', 'it', 'probabl', 'not']\n",
      "Spelled, stemmed and lemmatized (stopword removed): ['talk', 'driver', 'im', 'go', 'said', 'hed', 'love', 'go', 'new', 'york', 'sinc', 'trump', 'probabl']\n"
     ]
    }
   ],
   "source": [
    "tweet = df.iloc[1]\n",
    "print(f'Raw tweet (type {tweet[\"type\"]}): {tweet[\"text\"]}')\n",
    "\n",
    "for name, methods in preprocessing_methods.items():\n",
    "    tokenizer = get_tokenizer(methods)\n",
    "    data = tokenizer(tweet['text'])\n",
    "    print(f'{name}: {data}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 10 similar pairs of tweets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14min 39s, sys: 1.69 s, total: 14min 41s\n",
      "Wall time: 14min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prepared_vectorizers_transforms = dict()\n",
    "\n",
    "for name, methods in preprocessing_methods.items():\n",
    "    tokenizer = get_tokenizer(methods)\n",
    "    vectorizer = TfidfVectorizer(tokenizer=lambda x: tokenizer(x))\n",
    "    prepared_vectorizers_transforms[name] = vectorizer.fit_transform(df['text'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just tokenized\n",
      "\n",
      "0.9549145190462752\n",
      "share the love thanks for being top new followers this week happy want this\n",
      "share the love thanks for being top new followers this week happy want it\n",
      "\n",
      "0.9801880845327963\n",
      "hey thanks for being top new followers this week much appreciated happy want this\n",
      "hey thanks for being my top new followers this week much appreciated happy want this\n",
      "\n",
      "0.9626966357693176\n",
      "and more also in the epaper\n",
      "and more also in epaper\n",
      "\n",
      "0.961753097518891\n",
      "tamil nadu\n",
      "in tamil nadu\n",
      "\n",
      "0.9805362530533835\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "\n",
      "0.9944618246693316\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "\n",
      "0.9801880845327963\n",
      "hey thanks for being my top new followers this week much appreciated happy want this\n",
      "hey thanks for being top new followers this week much appreciated happy want this\n",
      "\n",
      "0.9944618246693316\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "\n",
      "0.9626966357693176\n",
      "and more also in epaper\n",
      "and more also in the epaper\n",
      "\n",
      "0.961753097518891\n",
      "in tamil nadu\n",
      "tamil nadu\n",
      "\n",
      "ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡\n",
      "\n",
      "Stemmed\n",
      "\n",
      "0.961753097518891\n",
      "tamil nadu\n",
      "in tamil nadu\n",
      "\n",
      "0.9626966357693176\n",
      "and more also in epaper\n",
      "and more also in the epaper\n",
      "\n",
      "0.9779831750309851\n",
      "hey thanks for being my top new followers this week much appreciated happy want this\n",
      "hey thanks for being top new followers this week much appreciated happy want this\n",
      "\n",
      "0.9626966357693176\n",
      "and more also in the epaper\n",
      "and more also in epaper\n",
      "\n",
      "0.9779831750309851\n",
      "hey thanks for being top new followers this week much appreciated happy want this\n",
      "hey thanks for being my top new followers this week much appreciated happy want this\n",
      "\n",
      "0.9908282955299917\n",
      "stats for the day have arrived to new followers and no unfollowers happy via\n",
      "stats for the day have arrived new follower and no unfollowers happy via\n",
      "\n",
      "0.9908282955299917\n",
      "stats for the day have arrived new follower and no unfollowers happy via\n",
      "stats for the day have arrived to new followers and no unfollowers happy via\n",
      "\n",
      "0.9939359992201824\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "\n",
      "0.9939359992201824\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "\n",
      "0.978785412768762\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "\n",
      "ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡\n",
      "\n",
      "Stemmed (stopword removed)\n",
      "\n",
      "0.9599463863210744\n",
      "thanks for the recent follow happy to connect happy have a great thursday\n",
      "thanks for the recent follow happy to connect happy have a great thursday want it\n",
      "\n",
      "0.9697125310567251\n",
      "thanks for the recent follow happy to connect happy have a great thursday want it\n",
      "thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "\n",
      "0.9697125310567251\n",
      "thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "thanks for the recent follow happy to connect happy have a great thursday want it\n",
      "\n",
      "0.9697747597987035\n",
      "thanks for the recent follow happy to connect happy have a great thursday get it\n",
      "thanks for the recent follow happy to connect happy have a great thursday get this\n",
      "\n",
      "0.9697747597987035\n",
      "thanks for the recent follow happy to connect happy have a great thursday get this\n",
      "thanks for the recent follow happy to connect happy have a great thursday get it\n",
      "\n",
      "0.9787315127515039\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "\n",
      "0.9734941561829112\n",
      "share the love thanks for being top new followers this week happy want this\n",
      "share the love thanks for being top new followers this week happy want it\n",
      "\n",
      "0.9787315127515039\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "\n",
      "0.9787315127515039\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "\n",
      "0.9734941561829112\n",
      "share the love thanks for being top new followers this week happy want it\n",
      "share the love thanks for being top new followers this week happy want this\n",
      "\n",
      "ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡\n",
      "\n",
      "Lemmatized\n",
      "\n",
      "0.9626966357693176\n",
      "and more also in epaper\n",
      "and more also in the epaper\n",
      "\n",
      "0.9916381579487277\n",
      "stats for the day have arrived new follower and no unfollowers happy via\n",
      "stats for the day have arrived to new followers and no unfollowers happy via\n",
      "\n",
      "0.9999999999999998\n",
      "babies unhappy\n",
      "baby unhappy\n",
      "\n",
      "0.9799738262861636\n",
      "hey thanks for being my top new followers this week much appreciated happy want this\n",
      "hey thanks for being top new followers this week much appreciated happy want this\n",
      "\n",
      "0.9999999999999998\n",
      "baby unhappy\n",
      "babies unhappy\n",
      "\n",
      "0.9941735336111193\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "\n",
      "0.9941735336111193\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "\n",
      "0.9803688174467615\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "\n",
      "0.9799738262861636\n",
      "hey thanks for being top new followers this week much appreciated happy want this\n",
      "hey thanks for being my top new followers this week much appreciated happy want this\n",
      "\n",
      "0.9916381579487277\n",
      "stats for the day have arrived to new followers and no unfollowers happy via\n",
      "stats for the day have arrived new follower and no unfollowers happy via\n",
      "\n",
      "ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡\n",
      "\n",
      "Lemmatized (stopword removed)\n",
      "\n",
      "0.9641937784491623\n",
      "thanks for the recent follow happy to connect happy have a great wednesday\n",
      "thanks for the recent follow happy to connect happy have a great wednesday want this\n",
      "\n",
      "0.9641937784491623\n",
      "thanks for the recent follow happy to connect happy have a great wednesday want this\n",
      "thanks for the recent follow happy to connect happy have a great wednesday\n",
      "\n",
      "0.964242011640493\n",
      "hey thanks for being my top new followers this week much appreciated happy want this\n",
      "hey thanks for being top new followers this week much appreciated happy\n",
      "\n",
      "0.964242011640493\n",
      "hey thanks for being top new followers this week much appreciated happy\n",
      "hey thanks for being top new followers this week much appreciated happy want this\n",
      "\n",
      "0.9806357428852037\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "\n",
      "0.9806357428852037\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "\n",
      "0.9806357428852037\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "\n",
      "0.9999999999999998\n",
      "baby unhappy\n",
      "babies unhappy\n",
      "\n",
      "0.9999999999999998\n",
      "babies unhappy\n",
      "baby unhappy\n",
      "\n",
      "0.964242011640493\n",
      "hey thanks for being top new followers this week much appreciated happy want this\n",
      "hey thanks for being top new followers this week much appreciated happy\n",
      "\n",
      "ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡\n",
      "\n",
      "Spelled\n",
      "\n",
      "0.9546290916474496\n",
      "share the love thanks for being top new followers this week happy want this\n",
      "share the love thanks for being top new followers this week happy want it\n",
      "\n",
      "0.961753097518891\n",
      "tamil nadu\n",
      "in tamil nadu\n",
      "\n",
      "0.9944482514225581\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "\n",
      "0.9806068081157996\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "\n",
      "0.9625611813740301\n",
      "and more also in epaper\n",
      "and more also in the epaper\n",
      "\n",
      "0.9944482514225581\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "\n",
      "0.9625611813740301\n",
      "and more also in the epaper\n",
      "and more also in epaper\n",
      "\n",
      "0.9800807533424065\n",
      "hey thanks for being my top new followers this week much appreciated happy want this\n",
      "hey thanks for being top new followers this week much appreciated happy want this\n",
      "\n",
      "0.9800807533424065\n",
      "hey thanks for being top new followers this week much appreciated happy want this\n",
      "hey thanks for being my top new followers this week much appreciated happy want this\n",
      "\n",
      "0.961753097518891\n",
      "in tamil nadu\n",
      "tamil nadu\n",
      "\n",
      "ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡\n",
      "\n",
      "Spelled (stopword removed)\n",
      "\n",
      "0.9625582819092171\n",
      "thanks for the recent follow happy to connect happy have a great wednesday want this\n",
      "thanks for the recent follow happy to connect happy have a great wednesday\n",
      "\n",
      "0.9628739900932984\n",
      "hey thanks for being top new followers this week much appreciated happy\n",
      "hey thanks for being top new followers this week much appreciated happy want this\n",
      "\n",
      "0.9628739900932984\n",
      "hey thanks for being top new followers this week much appreciated happy want this\n",
      "hey thanks for being top new followers this week much appreciated happy\n",
      "\n",
      "0.9628739900932984\n",
      "hey thanks for being my top new followers this week much appreciated happy want this\n",
      "hey thanks for being top new followers this week much appreciated happy\n",
      "\n",
      "0.9809238489747437\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "\n",
      "0.9999999999999996\n",
      "thanks for the recent follow much appreciated happy get it\n",
      "thanks for the recent follow much appreciated happy get this\n",
      "\n",
      "0.9809238489747437\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "\n",
      "0.9809238489747437\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "\n",
      "0.9625582819092171\n",
      "thanks for the recent follow happy to connect happy have a great wednesday\n",
      "thanks for the recent follow happy to connect happy have a great wednesday want this\n",
      "\n",
      "0.9999999999999996\n",
      "thanks for the recent follow much appreciated happy get this\n",
      "thanks for the recent follow much appreciated happy get it\n",
      "\n",
      "ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡\n",
      "\n",
      "Spelled and lemmatized\n",
      "\n",
      "0.961753097518891\n",
      "tamil nadu\n",
      "in tamil nadu\n",
      "\n",
      "0.9625611813740301\n",
      "and more also in epaper\n",
      "and more also in the epaper\n",
      "\n",
      "0.9625611813740301\n",
      "and more also in the epaper\n",
      "and more also in epaper\n",
      "\n",
      "0.9799318028473405\n",
      "hey thanks for being my top new followers this week much appreciated happy want this\n",
      "hey thanks for being top new followers this week much appreciated happy want this\n",
      "\n",
      "0.9941575839718166\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "\n",
      "0.9804220881788763\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "\n",
      "0.9941575839718166\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "\n",
      "0.9799318028473405\n",
      "hey thanks for being top new followers this week much appreciated happy want this\n",
      "hey thanks for being my top new followers this week much appreciated happy want this\n",
      "\n",
      "0.9926122881131061\n",
      "stats for the day have arrived to new followers and no unfollowers happy via\n",
      "stats for the day have arrived new follower and no unfollowers happy via\n",
      "\n",
      "0.9926122881131061\n",
      "stats for the day have arrived new follower and no unfollowers happy via\n",
      "stats for the day have arrived to new followers and no unfollowers happy via\n",
      "\n",
      "ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡\n",
      "\n",
      "Spelled and lemmatized (stopword removed)\n",
      "\n",
      "0.9614506765773134\n",
      "thanks for the recent follow happy to connect happy have a great thursday want it\n",
      "thanks for the recent follow happy to connect happy have a great thursday\n",
      "\n",
      "0.9614506765773134\n",
      "thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "thanks for the recent follow happy to connect happy have a great thursday\n",
      "\n",
      "0.9643811423130969\n",
      "thanks for the recent follow happy to connect happy have a great wednesday\n",
      "thanks for the recent follow happy to connect happy have a great wednesday want this\n",
      "\n",
      "0.9643811423130969\n",
      "thanks for the recent follow happy to connect happy have a great wednesday want this\n",
      "thanks for the recent follow happy to connect happy have a great wednesday\n",
      "\n",
      "0.9644408022758854\n",
      "hey thanks for being my top new followers this week much appreciated happy want this\n",
      "hey thanks for being top new followers this week much appreciated happy\n",
      "\n",
      "0.9644408022758854\n",
      "hey thanks for being top new followers this week much appreciated happy\n",
      "hey thanks for being top new followers this week much appreciated happy want this\n",
      "\n",
      "0.9644408022758854\n",
      "hey thanks for being top new followers this week much appreciated happy want this\n",
      "hey thanks for being top new followers this week much appreciated happy\n",
      "\n",
      "0.9807142922446532\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "\n",
      "0.9807142922446532\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "\n",
      "0.9807142922446532\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "\n",
      "ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡\n",
      "\n",
      "Spelled and stemmed\n",
      "\n",
      "0.961753097518891\n",
      "in tamil nadu\n",
      "tamil nadu\n",
      "\n",
      "0.9625611813740301\n",
      "and more also in the epaper\n",
      "and more also in epaper\n",
      "\n",
      "0.9625611813740301\n",
      "and more also in epaper\n",
      "and more also in the epaper\n",
      "\n",
      "0.9779059574028904\n",
      "hey thanks for being top new followers this week much appreciated happy want this\n",
      "hey thanks for being my top new followers this week much appreciated happy want this\n",
      "\n",
      "0.978806814962104\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "\n",
      "0.9779059574028904\n",
      "hey thanks for being my top new followers this week much appreciated happy want this\n",
      "hey thanks for being top new followers this week much appreciated happy want this\n",
      "\n",
      "0.9912332154843165\n",
      "stats for the day have arrived new follower and no unfollowers happy via\n",
      "stats for the day have arrived to new followers and no unfollowers happy via\n",
      "\n",
      "0.9939043175454587\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "\n",
      "0.9912332154843165\n",
      "stats for the day have arrived to new followers and no unfollowers happy via\n",
      "stats for the day have arrived new follower and no unfollowers happy via\n",
      "\n",
      "0.9939043175454587\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "\n",
      "ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡\n",
      "\n",
      "Spelled and stemmed (stopword removed)\n",
      "\n",
      "0.9600914449136776\n",
      "thanks for the recent follow happy to connect happy have a great thursday\n",
      "thanks for the recent follow happy to connect happy have a great thursday want it\n",
      "\n",
      "0.9787744547457674\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "\n",
      "0.9696762295006242\n",
      "thanks for the recent follow happy to connect happy have a great thursday want it\n",
      "thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "\n",
      "0.9787744547457674\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "\n",
      "0.9733147867396652\n",
      "share the love thanks for being top new followers this week happy want this\n",
      "share the love thanks for being top new followers this week happy want it\n",
      "\n",
      "0.9733147867396652\n",
      "share the love thanks for being top new followers this week happy want it\n",
      "share the love thanks for being top new followers this week happy want this\n",
      "\n",
      "0.9787744547457674\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "\n",
      "0.9696762295006242\n",
      "thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "thanks for the recent follow happy to connect happy have a great thursday want it\n",
      "\n",
      "0.9697466039287853\n",
      "thanks for the recent follow happy to connect happy have a great thursday get this\n",
      "thanks for the recent follow happy to connect happy have a great thursday get it\n",
      "\n",
      "0.9697466039287853\n",
      "thanks for the recent follow happy to connect happy have a great thursday get it\n",
      "thanks for the recent follow happy to connect happy have a great thursday get this\n",
      "\n",
      "ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡\n",
      "\n",
      "Spelled, stemmed and lemmatized\n",
      "\n",
      "0.961753097518891\n",
      "in tamil nadu\n",
      "tamil nadu\n",
      "\n",
      "0.9625611813740301\n",
      "and more also in the epaper\n",
      "and more also in epaper\n",
      "\n",
      "0.9625611813740301\n",
      "and more also in epaper\n",
      "and more also in the epaper\n",
      "\n",
      "0.9779059574028904\n",
      "hey thanks for being top new followers this week much appreciated happy want this\n",
      "hey thanks for being my top new followers this week much appreciated happy want this\n",
      "\n",
      "0.9779059574028904\n",
      "hey thanks for being my top new followers this week much appreciated happy want this\n",
      "hey thanks for being top new followers this week much appreciated happy want this\n",
      "\n",
      "0.993889617520387\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "\n",
      "0.9912332154843165\n",
      "stats for the day have arrived to new followers and no unfollowers happy via\n",
      "stats for the day have arrived new follower and no unfollowers happy via\n",
      "\n",
      "0.9912332154843165\n",
      "stats for the day have arrived new follower and no unfollowers happy via\n",
      "stats for the day have arrived to new followers and no unfollowers happy via\n",
      "\n",
      "0.993889617520387\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "\n",
      "0.9795705337159327\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "\n",
      "ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡\n",
      "\n",
      "Spelled, stemmed and lemmatized (stopword removed)\n",
      "\n",
      "0.9600914449136776\n",
      "thanks for the recent follow happy to connect happy have a great thursday\n",
      "thanks for the recent follow happy to connect happy have a great thursday want it\n",
      "\n",
      "0.9696762295006242\n",
      "thanks for the recent follow happy to connect happy have a great thursday want it\n",
      "thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "\n",
      "0.9697466039287853\n",
      "thanks for the recent follow happy to connect happy have a great thursday get this\n",
      "thanks for the recent follow happy to connect happy have a great thursday get it\n",
      "\n",
      "0.9696762295006242\n",
      "thanks for the recent follow happy to connect happy have a great thursday want this\n",
      "thanks for the recent follow happy to connect happy have a great thursday want it\n",
      "\n",
      "0.9697466039287853\n",
      "thanks for the recent follow happy to connect happy have a great thursday get it\n",
      "thanks for the recent follow happy to connect happy have a great thursday get this\n",
      "\n",
      "0.9797217891276386\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "\n",
      "0.9733147867396652\n",
      "share the love thanks for being top new followers this week happy want this\n",
      "share the love thanks for being top new followers this week happy want it\n",
      "\n",
      "0.9797217891276386\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number cont\n",
      "\n",
      "0.9733147867396652\n",
      "share the love thanks for being top new followers this week happy want it\n",
      "share the love thanks for being top new followers this week happy want this\n",
      "\n",
      "0.9797217891276386\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for cont\n",
      "hi we tried to call your number but got no response unhappy please share another suitable time and an alternate number for us to cont\n",
      "\n",
      "ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡\n",
      "\n",
      "CPU times: user 1.82 s, sys: 188 ms, total: 2.01 s\n",
      "Wall time: 1.92 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def find_top_similarity_tweets(similarity_tweets_matrix):\n",
    "    N = 10\n",
    "    i = 0\n",
    "\n",
    "    similarity_index_array = np.zeros(shape=(len(similarity_tweets_matrix)), dtype=np.int16)\n",
    "    similarity_value_array = np.zeros(shape=(len(similarity_tweets_matrix)))\n",
    "\n",
    "    while i < len(similarity_tweets_matrix):\n",
    "        array = np.array(similarity_tweets_matrix[i])\n",
    "        array[i] = 0.0\n",
    "        array[array >= 1.0] = 0.0\n",
    "        max_index = np.argmax(array)\n",
    "        similarity_index_array[i] = max_index\n",
    "        similarity_value_array[i] = array[max_index]\n",
    "        i += 1\n",
    "\n",
    "    top_indexes = np.argpartition(similarity_value_array, -N)[-N:]\n",
    "\n",
    "    for index in top_indexes:\n",
    "        print(similarity_value_array[index])\n",
    "        print(df.iloc[index]['text'])\n",
    "        print(df.iloc[similarity_index_array[index]]['text'])\n",
    "        print('')\n",
    "\n",
    "\n",
    "for name, transform in prepared_vectorizers_transforms.items():\n",
    "    print(f'{name}\\n')\n",
    "    similarity_matrix = cosine_similarity(transform)\n",
    "    find_top_similarity_tweets(similarity_matrix)\n",
    "    print(f'{OUTPUT_SEPARATOR}\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Choose best preprocessing method"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "def choose_best_preprocessing_method(methods_dict):\n",
    "    best_method_name = ''\n",
    "    best_vectorizer = ''\n",
    "    best_accuracy = 0\n",
    "    for name, methods in methods_dict.items():\n",
    "        tokenizer = get_tokenizer(methods)\n",
    "\n",
    "        bin_vectorizer = CountVectorizer(tokenizer=lambda x: tokenizer(x), binary=True)\n",
    "        count_vectorizer = CountVectorizer(tokenizer=lambda x: tokenizer(x))\n",
    "        tfidf_vectorizer = TfidfVectorizer(tokenizer=lambda x: tokenizer(x))\n",
    "\n",
    "        model = RandomForestClassifier(\n",
    "            class_weight='balanced',\n",
    "            criterion='entropy',\n",
    "            max_depth=170,\n",
    "            n_estimators=200,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        grid_pipline_bin_vec = Pipeline([\n",
    "            ('vectorizer', bin_vectorizer),\n",
    "            ('model', model),\n",
    "        ])\n",
    "\n",
    "        grid_pipline_count_vec = Pipeline([\n",
    "            ('vectorizer', count_vectorizer),\n",
    "            ('model', model),\n",
    "        ])\n",
    "\n",
    "        grid_pipline_tfidf_vec = Pipeline([\n",
    "            ('vectorizer', tfidf_vectorizer),\n",
    "            ('model', model),\n",
    "        ])\n",
    "\n",
    "        grid_pipline_bin_vec.fit(train_df['text'], train_df['type'])\n",
    "        grid_pipline_count_vec.fit(train_df['text'], train_df['type'])\n",
    "        grid_pipline_tfidf_vec.fit(train_df['text'], train_df['type'])\n",
    "\n",
    "        y_pred_bin_vect = grid_pipline_bin_vec.predict(test_df['text'])\n",
    "        y_pred_count_vect = grid_pipline_count_vec.predict(test_df['text'])\n",
    "        y_pred_tfidf_vect = grid_pipline_tfidf_vec.predict(test_df['text'])\n",
    "        y_true = test_df['type']\n",
    "\n",
    "        accuracy_bin_vec = accuracy_score(y_true, y_pred_bin_vect)\n",
    "        accuracy_count_vec = accuracy_score(y_true, y_pred_count_vect)\n",
    "        accuracy_tfidf_vec = accuracy_score(y_true, y_pred_tfidf_vect)\n",
    "\n",
    "        current_best_accuracy = 0\n",
    "        current_best_vectorizer = ''\n",
    "        if accuracy_bin_vec > current_best_accuracy:\n",
    "            current_best_accuracy = accuracy_bin_vec\n",
    "            current_best_vectorizer = 'bin vectorizer'\n",
    "        elif accuracy_count_vec > current_best_accuracy:\n",
    "            current_best_accuracy = accuracy_count_vec\n",
    "            current_best_vectorizer = 'count vectorizer'\n",
    "        elif accuracy_tfidf_vec > current_best_accuracy:\n",
    "            current_best_accuracy = accuracy_tfidf_vec\n",
    "            current_best_vectorizer = 'tfidf vectorizer'\n",
    "\n",
    "        print(f'Preprocessing methods: {name}')\n",
    "        print(f'accuracy bin vectorizer: {accuracy_bin_vec}')\n",
    "        print(f'accuracy count vectorizer: {accuracy_count_vec}')\n",
    "        print(f'accuracy tfidf vectorizer: {accuracy_tfidf_vec}\\n')\n",
    "\n",
    "        if current_best_accuracy > best_accuracy:\n",
    "            best_accuracy = current_best_accuracy\n",
    "            best_method_name = name\n",
    "            best_vectorizer = current_best_vectorizer\n",
    "\n",
    "    return {'name': best_method_name, 'vectorizer': best_vectorizer, 'accuracy': best_accuracy}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing methods: Just tokenized\n",
      "accuracy bin vectorizer: 0.8851612903225806\n",
      "accuracy count vectorizer: 0.8851612903225806\n",
      "accuracy tfidf vectorizer: 0.8941935483870967\n",
      "\n",
      "Preprocessing methods: Stemmed\n",
      "accuracy bin vectorizer: 0.8903225806451613\n",
      "accuracy count vectorizer: 0.8903225806451613\n",
      "accuracy tfidf vectorizer: 0.895483870967742\n",
      "\n",
      "Preprocessing methods: Stemmed (stopword removed)\n",
      "accuracy bin vectorizer: 0.8761290322580645\n",
      "accuracy count vectorizer: 0.8761290322580645\n",
      "accuracy tfidf vectorizer: 0.8877419354838709\n",
      "\n",
      "Preprocessing methods: Lemmatized\n",
      "accuracy bin vectorizer: 0.8890322580645161\n",
      "accuracy count vectorizer: 0.8890322580645161\n",
      "accuracy tfidf vectorizer: 0.8941935483870967\n",
      "\n",
      "Preprocessing methods: Lemmatized (stopword removed)\n",
      "accuracy bin vectorizer: 0.8838709677419355\n",
      "accuracy count vectorizer: 0.8838709677419355\n",
      "accuracy tfidf vectorizer: 0.8877419354838709\n",
      "\n",
      "Preprocessing methods: Spelled\n",
      "accuracy bin vectorizer: 0.8916129032258064\n",
      "accuracy count vectorizer: 0.8916129032258064\n",
      "accuracy tfidf vectorizer: 0.8993548387096775\n",
      "\n",
      "Preprocessing methods: Spelled (stopword removed)\n",
      "accuracy bin vectorizer: 0.8851612903225806\n",
      "accuracy count vectorizer: 0.8851612903225806\n",
      "accuracy tfidf vectorizer: 0.8903225806451613\n",
      "\n",
      "Preprocessing methods: Spelled and lemmatized\n",
      "accuracy bin vectorizer: 0.8903225806451613\n",
      "accuracy count vectorizer: 0.8903225806451613\n",
      "accuracy tfidf vectorizer: 0.8980645161290323\n",
      "\n",
      "Preprocessing methods: Spelled and lemmatized (stopword removed)\n",
      "accuracy bin vectorizer: 0.8825806451612903\n",
      "accuracy count vectorizer: 0.8825806451612903\n",
      "accuracy tfidf vectorizer: 0.8916129032258064\n",
      "\n",
      "Preprocessing methods: Spelled and stemmed\n",
      "accuracy bin vectorizer: 0.8993548387096775\n",
      "accuracy count vectorizer: 0.8993548387096775\n",
      "accuracy tfidf vectorizer: 0.8993548387096775\n",
      "\n",
      "Preprocessing methods: Spelled and stemmed (stopword removed)\n",
      "accuracy bin vectorizer: 0.8812903225806452\n",
      "accuracy count vectorizer: 0.8812903225806452\n",
      "accuracy tfidf vectorizer: 0.8877419354838709\n",
      "\n",
      "Preprocessing methods: Spelled, stemmed and lemmatized\n",
      "accuracy bin vectorizer: 0.896774193548387\n",
      "accuracy count vectorizer: 0.896774193548387\n",
      "accuracy tfidf vectorizer: 0.9019354838709678\n",
      "\n",
      "Preprocessing methods: Spelled, stemmed and lemmatized (stopword removed)\n",
      "accuracy bin vectorizer: 0.8787096774193548\n",
      "accuracy count vectorizer: 0.8787096774193548\n",
      "accuracy tfidf vectorizer: 0.8903225806451613\n",
      "\n",
      "Best preprocessing method: {'name': 'Spelled and stemmed', 'vectorizer': 'bin vectorizer', 'accuracy': 0.8993548387096775}\n",
      "ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡\n",
      "CPU times: user 34min 9s, sys: 3.93 s, total: 34min 13s\n",
      "Wall time: 34min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_method = choose_best_preprocessing_method(preprocessing_methods)\n",
    "\n",
    "print(f'Best preprocessing method: {best_method}')\n",
    "print(OUTPUT_SEPARATOR)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get best params"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 380 µs, sys: 2 µs, total: 382 µs\n",
      "Wall time: 389 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [1, 10, 30, 50, 70, 100, 130, 150, 170, 190, 230],\n",
    "    'n_estimators': (5, 10, 20, 40, 60, 100, 150, 200, 300),\n",
    "    'criterion': ('entropy', 'gini'),\n",
    "    'class_weight': ['balanced', None]\n",
    "}\n",
    "\n",
    "methods = preprocessing_methods['Spelled and stemmed']\n",
    "tokenizer = get_tokenizer(methods)\n",
    "\n",
    "bin_vectorizer = CountVectorizer(tokenizer=lambda x: tokenizer(x), binary=True)\n",
    "count_vectorizer = CountVectorizer(tokenizer=lambda x: tokenizer(x))\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=lambda x: tokenizer(x))\n",
    "\n",
    "grid_pipline_model = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=0),\n",
    "    param_grid=param_grid,\n",
    "    cv=4,\n",
    "    verbose=2,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_pipline_bin_vectorizer = Pipeline([\n",
    "    ('vectorizer', bin_vectorizer),\n",
    "    ('model', grid_pipline_model),\n",
    "])\n",
    "\n",
    "grid_pipline_count_vectorizer = Pipeline([\n",
    "    ('vectorizer', count_vectorizer),\n",
    "    ('model', grid_pipline_model),\n",
    "])\n",
    "\n",
    "grid_pipline_tfidf_vectorizer = Pipeline([\n",
    "    ('vectorizer', tfidf_vectorizer),\n",
    "    ('model', grid_pipline_model),\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 396 candidates, totalling 1584 fits\n",
      "Fitting 4 folds for each of 396 candidates, totalling 1584 fits\n",
      "Fitting 4 folds for each of 396 candidates, totalling 1584 fits\n",
      "Bin vectorizer:\n",
      "best params: {'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 190, 'n_estimators': 200}\n",
      "accuracy: 0.9045161290322581\n",
      "Count vectorizer:\n",
      "best params: {'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 190, 'n_estimators': 200}\n",
      "accuracy: 0.9045161290322581\n",
      "TFIDF vectorizer:\n",
      "best params: {'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 190, 'n_estimators': 200}\n",
      "accuracy: 0.9006451612903226\n",
      "ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡\n",
      "CPU times: user 7min 26s, sys: 4.42 s, total: 7min 31s\n",
      "Wall time: 33min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "grid_pipline_bin_vectorizer.fit(train_df['text'], train_df['type'])\n",
    "grid_pipline_count_vectorizer.fit(train_df['text'], train_df['type'])\n",
    "grid_pipline_tfidf_vectorizer.fit(train_df['text'], train_df['type'])\n",
    "\n",
    "best_params_bin_vectorizer = grid_pipline_bin_vectorizer['model'].best_params_\n",
    "best_params_count_vectorizer = grid_pipline_count_vectorizer['model'].best_params_\n",
    "best_params_tfidf_vectorizer = grid_pipline_tfidf_vectorizer['model'].best_params_\n",
    "\n",
    "y_pred_bin_vectorizer = grid_pipline_bin_vectorizer.predict(test_df['text'])\n",
    "y_pred_count_vectorizer = grid_pipline_count_vectorizer.predict(test_df['text'])\n",
    "y_pred_tfidf_vectorizer = grid_pipline_tfidf_vectorizer.predict(test_df['text'])\n",
    "y_true = test_df['type']\n",
    "\n",
    "accuracy_bin_vectorizer = accuracy_score(y_true, y_pred_bin_vectorizer)\n",
    "accuracy_count_vectorizer = accuracy_score(y_true, y_pred_count_vectorizer)\n",
    "accuracy_tfidf_vectorizer = accuracy_score(y_true, y_pred_tfidf_vectorizer)\n",
    "\n",
    "print('Bin vectorizer:')\n",
    "print(f'best params: {best_params_bin_vectorizer}')\n",
    "print(f'accuracy: {accuracy_bin_vectorizer}')\n",
    "print('Count vectorizer:')\n",
    "print(f'best params: {best_params_count_vectorizer}')\n",
    "print(f'accuracy: {accuracy_count_vectorizer}')\n",
    "print('TFIDF vectorizer:')\n",
    "print(f'best params: {best_params_tfidf_vectorizer}')\n",
    "print(f'accuracy: {accuracy_tfidf_vectorizer}')\n",
    "print(OUTPUT_SEPARATOR)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 225 µs, sys: 10 µs, total: 235 µs\n",
      "Wall time: 241 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': list(range(180, 200)),\n",
    "    'n_estimators': list(range(190, 210)),\n",
    "}\n",
    "\n",
    "bin_vectorizer = CountVectorizer(tokenizer=lambda x: tokenizer(x), binary=True)\n",
    "count_vectorizer = CountVectorizer(tokenizer=lambda x: tokenizer(x))\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=lambda x: tokenizer(x))\n",
    "\n",
    "grid_pipline_model = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=0, class_weight='balanced'),\n",
    "    param_grid=param_grid,\n",
    "    cv=4,\n",
    "    verbose=2,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_pipline_bin_vectorizer = Pipeline([\n",
    "    ('vectorizer', bin_vectorizer),\n",
    "    ('model', grid_pipline_model),\n",
    "])\n",
    "\n",
    "grid_pipline_count_vectorizer = Pipeline([\n",
    "    ('vectorizer', count_vectorizer),\n",
    "    ('model', grid_pipline_model),\n",
    "])\n",
    "\n",
    "grid_pipline_tfidf_vectorizer = Pipeline([\n",
    "    ('vectorizer', tfidf_vectorizer),\n",
    "    ('model', grid_pipline_model),\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 400 candidates, totalling 1600 fits\n",
      "Fitting 4 folds for each of 400 candidates, totalling 1600 fits\n",
      "Fitting 4 folds for each of 400 candidates, totalling 1600 fits\n",
      "Bin vectorizer:\n",
      "best params: {'max_depth': 189, 'n_estimators': 195}\n",
      "accuracy: 0.9006451612903226\n",
      "Count vectorizer:\n",
      "best params: {'max_depth': 189, 'n_estimators': 195}\n",
      "accuracy: 0.9006451612903226\n",
      "TFIDF vectorizer:\n",
      "best params: {'max_depth': 189, 'n_estimators': 195}\n",
      "accuracy: 0.9019354838709678\n",
      "ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡ ¯\\_(ツ)_/¯ (─‿‿─) 〈( ^.^)ノ ʕ•́ᴥ•̀ʔっ♡\n",
      "CPU times: user 6min 58s, sys: 4.34 s, total: 7min 2s\n",
      "Wall time: 1h 6min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "grid_pipline_bin_vectorizer.fit(train_df['text'], train_df['type'])\n",
    "grid_pipline_count_vectorizer.fit(train_df['text'], train_df['type'])\n",
    "grid_pipline_tfidf_vectorizer.fit(train_df['text'], train_df['type'])\n",
    "\n",
    "best_params_bin_vectorizer = grid_pipline_bin_vectorizer['model'].best_params_\n",
    "best_params_count_vectorizer = grid_pipline_count_vectorizer['model'].best_params_\n",
    "best_params_tfidf_vectorizer = grid_pipline_tfidf_vectorizer['model'].best_params_\n",
    "\n",
    "y_pred_bin_vectorizer = grid_pipline_bin_vectorizer.predict(test_df['text'])\n",
    "y_pred_count_vectorizer = grid_pipline_count_vectorizer.predict(test_df['text'])\n",
    "y_pred_tfidf_vectorizer = grid_pipline_tfidf_vectorizer.predict(test_df['text'])\n",
    "y_true = test_df['type']\n",
    "\n",
    "accuracy_bin_vectorizer = accuracy_score(y_true, y_pred_bin_vectorizer)\n",
    "accuracy_count_vectorizer = accuracy_score(y_true, y_pred_count_vectorizer)\n",
    "accuracy_tfidf_vectorizer = accuracy_score(y_true, y_pred_tfidf_vectorizer)\n",
    "\n",
    "print('Bin vectorizer:')\n",
    "print(f'best params: {best_params_bin_vectorizer}')\n",
    "print(f'accuracy: {accuracy_bin_vectorizer}')\n",
    "print('Count vectorizer:')\n",
    "print(f'best params: {best_params_count_vectorizer}')\n",
    "print(f'accuracy: {accuracy_count_vectorizer}')\n",
    "print('TFIDF vectorizer:')\n",
    "print(f'best params: {best_params_tfidf_vectorizer}')\n",
    "print(f'accuracy: {accuracy_tfidf_vectorizer}')\n",
    "print(OUTPUT_SEPARATOR)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Different classification approaches"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "outputs": [],
   "source": [
    "methods = preprocessing_methods['Spelled and stemmed']\n",
    "tokenizer = get_tokenizer(methods)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Random forest with best params"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 42s, sys: 179 ms, total: 1min 42s\n",
      "Wall time: 1min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=lambda x: tokenizer(x))\n",
    "model = RandomForestClassifier(random_state=0, class_weight='balanced', max_depth=189, n_estimators=195)\n",
    "\n",
    "pipline = Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('model', model),\n",
    "])\n",
    "\n",
    "pipline = pipline.fit(train_df['text'], train_df['type'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8976608187134503\n",
      "CPU times: user 17.9 s, sys: 19.6 ms, total: 18 s\n",
      "Wall time: 18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "y_true = test_df['type']\n",
    "y_pred = pipline.predict(test_df['text'])\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "print(f'accuracy: {accuracy}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### CatBoost"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 14s, sys: 6.56 s, total: 5min 21s\n",
      "Wall time: 2min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=lambda x: tokenizer(x))\n",
    "model = CatBoostClassifier()\n",
    "\n",
    "pipline = Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('model', model),\n",
    "])\n",
    "\n",
    "pipline = pipline.fit(train_df['text'], train_df['type'], model__silent=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8903508771929824\n",
      "CPU times: user 35.5 s, sys: 109 ms, total: 35.6 s\n",
      "Wall time: 35.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "y_true = test_df['type']\n",
    "y_pred = pipline.predict(test_df['text'])\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "print(f'accuracy: {accuracy}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word2vect"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 15s, sys: 1.11 s, total: 4min 16s\n",
      "Wall time: 4min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "methods = preprocessing_methods['Spelled and stemmed']\n",
    "tokenizer = get_tokenizer(methods)\n",
    "sentences = [tokenizer(tweet) for tweet in train_df['text']]\n",
    "test_sentences = [tokenizer(tweet) for tweet in test_df['text']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 47s, sys: 5.56 s, total: 4min 53s\n",
      "Wall time: 2min 44s\n"
     ]
    },
    {
     "data": {
      "text/plain": "(73544164, 92844000)"
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "num_features = 300\n",
    "\n",
    "model = Word2Vec(\n",
    "    workers=4,\n",
    "    vector_size=num_features,\n",
    "    min_count=1,\n",
    "    sample=1e-3\n",
    ")\n",
    "\n",
    "model.build_vocab(sentences)\n",
    "model.train(sentences, total_examples=len(sentences), epochs=4000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twitter [('sir', 0.3634018003940582), ('logotyp', 0.31645673513412476), ('tendulkar', 0.3134271502494812)]\n",
      "love [('miss', 0.3488241732120514), ('need', 0.2986733615398407), ('dont', 0.29372280836105347), ('glad', 0.2822946310043335), ('have', 0.2746826708316803)]\n",
      "trump [('defianc', 0.3880469799041748), ('sharp', 0.36645200848579407), ('salli', 0.35442009568214417), ('father', 0.34890905022621155), ('advis', 0.3322913348674774), ('lawyer', 0.3256087005138397), ('waterway', 0.3191172778606415), ('flame', 0.31123027205467224), ('tripl', 0.30751481652259827), ('era', 0.30501630902290344)]\n"
     ]
    }
   ],
   "source": [
    "print('twitter', model.wv.most_similar('twitter', topn=3))\n",
    "print('love', model.wv.most_similar('love', topn=5))\n",
    "print('trump', model.wv.most_similar('trump', topn=10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "outputs": [],
   "source": [
    "def make_feature_vec(words, model, num_features):\n",
    "    feature_vec = np.zeros((num_features,), dtype=\"float32\")\n",
    "    nwords = 0.\n",
    "    index2word_set = set(model.wv.index_to_key)\n",
    "\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1.\n",
    "            feature_vec = np.add(feature_vec, model.wv[word])\n",
    "\n",
    "    feature_vec = np.divide(feature_vec, nwords)\n",
    "    return feature_vec\n",
    "\n",
    "\n",
    "def get_avg_feature_vecs(reviews, model, num_features):\n",
    "    counter = 0\n",
    "    review_feature_vecs = np.zeros((len(reviews), num_features), dtype='float32')\n",
    "\n",
    "    for review in reviews:\n",
    "        review_feature_vecs[counter] = make_feature_vec(review, model, num_features)\n",
    "        counter = counter + 1\n",
    "    return review_feature_vecs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/17/kvk5ywwd5g7gfx6zjhfz859c0000gn/T/ipykernel_11093/3778286644.py:11: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec, nwords)\n"
     ]
    }
   ],
   "source": [
    "# calculate average feature vectors for training and test sets\n",
    "clean_train_reviews = []\n",
    "for review in sentences:\n",
    "    clean_train_reviews.append(review)\n",
    "trainDataVecs = get_avg_feature_vecs(clean_train_reviews, model, num_features)\n",
    "\n",
    "clean_test_reviews = []\n",
    "for review in test_sentences:\n",
    "    clean_test_reviews.append(review)\n",
    "testDataVecs = get_avg_feature_vecs(clean_test_reviews, model, num_features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "outputs": [],
   "source": [
    "trainDataVecs[np.isnan(trainDataVecs) == True] = 0.\n",
    "trainDataVecs[np.isinf(trainDataVecs) == True] = 0.\n",
    "testDataVecs[np.isnan(testDataVecs) == True] = 0.\n",
    "testDataVecs[np.isinf(testDataVecs) == True] = 0."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Random forest with best params"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.2 s, sys: 60.6 ms, total: 11.2 s\n",
      "Wall time: 12.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "forest = RandomForestClassifier(random_state=0, class_weight='balanced', max_depth=189, n_estimators=195)\n",
    "forest = forest.fit(trainDataVecs, train_df['type'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7587719298245614\n"
     ]
    }
   ],
   "source": [
    "y_true = test_df['type']\n",
    "y_pred = forest.predict(testDataVecs)\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "print(f'accuracy: {accuracy}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### CatBoost"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "outputs": [],
   "source": [
    "cat_boost = CatBoostClassifier()\n",
    "cat_boost = cat_boost.fit(trainDataVecs, train_df['type'], silent=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.814327485380117\n"
     ]
    }
   ],
   "source": [
    "y_true = test_df['type']\n",
    "y_pred = cat_boost.predict(testDataVecs)\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "print(f'accuracy: {accuracy}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}